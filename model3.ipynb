{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ALejandroCh17/Tennis-Ball-Robot/blob/daniel_branch/model3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the Dataset"
      ],
      "metadata": {
        "id": "UP83ylD-rLSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR_6xG0qLeSH",
        "outputId": "8eabcb2c-1275-4b66-d9c9-a7c6125cc6a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "base_dir = '/content/drive/My Drive/capstone_dataset_4/'\n",
        "\n",
        "train_dir = '/content/drive/My Drive/capstone_dataset_4/train'\n",
        "test_dir = '/content/drive/My Drive/capstone_dataset_4/test'\n",
        "valid_dir = '/content/drive/My Drive/capstone_dataset_4/valid'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qNSExIATBa63",
        "outputId": "8eb589dd-69e0-4eae-e25a-b67db44b76f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nbase_dir = '/content/drive/My Drive/capstone_dataset_4/'\\n\\ntrain_dir = '/content/drive/My Drive/capstone_dataset_4/train'\\ntest_dir = '/content/drive/My Drive/capstone_dataset_4/test'\\nvalid_dir = '/content/drive/My Drive/capstone_dataset_4/valid'\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import os\n",
        "\n",
        "trainFileCount = len([name for name in os.listdir(train_dir) if os.path.isfile(os.path.join(train_dir, name))])\n",
        "testFileCount = len([name for name in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, name))])\n",
        "validFileCount = len([name for name in os.listdir(valid_dir) if os.path.isfile(os.path.join(valid_dir, name))])\n",
        "\n",
        "print(\"Train count: \", str(trainFileCount))\n",
        "print(\"Test count: \", str(testFileCount))\n",
        "print(\"Valid count: \", str(validFileCount))\n",
        "print(\"Total: \", str(trainFileCount+testFileCount+validFileCount))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hfV68MUwPBKz",
        "outputId": "1d2c25cc-550a-461c-acb4-064273b3096e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport os\\n\\ntrainFileCount = len([name for name in os.listdir(train_dir) if os.path.isfile(os.path.join(train_dir, name))])\\ntestFileCount = len([name for name in os.listdir(test_dir) if os.path.isfile(os.path.join(test_dir, name))])\\nvalidFileCount = len([name for name in os.listdir(valid_dir) if os.path.isfile(os.path.join(valid_dir, name))])\\n\\nprint(\"Train count: \", str(trainFileCount))\\nprint(\"Test count: \", str(testFileCount))\\nprint(\"Valid count: \", str(validFileCount))\\nprint(\"Total: \", str(trainFileCount+testFileCount+validFileCount))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#combining all the data\n",
        "import shutil\n",
        "\n",
        "#create new directory to store total set\n",
        "totalDir = '/content/drive/My Drive/capstone_dataset_4/combined'\n",
        "os.makedirs(totalDir, exist_ok=True)\n",
        "\n",
        "#iterate over all 3 directories and copy files to totalDir\n",
        "for directory in [train_dir, test_dir, valid_dir]:\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.jpg'):\n",
        "            shutil.copy2(os.path.join(directory, filename), os.path.join(totalDir, filename))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "e81ZywQGMTo_",
        "outputId": "3779dcd3-5dfa-4e30-ac18-4167a2a2c703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#combining all the data\\nimport shutil\\n\\n#create new directory to store total set\\ntotalDir = '/content/drive/My Drive/capstone_dataset_4/combined'\\nos.makedirs(totalDir, exist_ok=True)\\n\\n#iterate over all 3 directories and copy files to totalDir\\nfor directory in [train_dir, test_dir, valid_dir]:\\n    for filename in os.listdir(directory):\\n        if filename.endswith('.jpg'):\\n            shutil.copy2(os.path.join(directory, filename), os.path.join(totalDir, filename))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "totalDir = '/content/drive/My Drive/capstone_dataset_4/combined'"
      ],
      "metadata": {
        "id": "zcHtV0WhtQrP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count the number of files in the new total directory\n",
        "totalFileCount = len([name for name in os.listdir(totalDir) if os.path.isfile(os.path.join(totalDir, name))])\n",
        "\n",
        "print(\"Total combined count: \", str(totalFileCount))"
      ],
      "metadata": {
        "id": "UWJwR1c9QSYy",
        "outputId": "c9176453-add0-4294-bf2a-254c4d16d763",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total combined count:  2548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removed 3 non images files, the JSON files"
      ],
      "metadata": {
        "id": "P0kXe4FGQWhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "JoY6A_AkrzFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check to see what size all the images by returning if they are all the same size.\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "def check_same_size(path):\n",
        "  sizes = set()\n",
        "  for filename in os.listdir(path):\n",
        "    if filename.endswith('.jpg'):\n",
        "      img = cv2.imread(os.path.join(path, filename))\n",
        "      if img is None:\n",
        "        print(f\"Image not loaded successfully: {filename}\")\n",
        "        continue\n",
        "      sizes.add(img.shape)\n",
        "\n",
        "  if len(sizes) == 1:\n",
        "    print(\"Yes, they are all the same size.\")\n",
        "  else:\n",
        "    print(\"No, they are not all the same size.\")\n",
        "\n",
        "check_same_size(totalDir)\n"
      ],
      "metadata": {
        "id": "TgMBdlbhrm60",
        "outputId": "0b73b303-d1e6-4963-da8a-0477812644c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, they are all the same size.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get the dimensions of one image\n",
        "\n",
        "import cv2\n",
        "\n",
        "# Read the first image in the directory\n",
        "image = cv2.imread(os.path.join(totalDir, os.listdir(totalDir)[0]))\n",
        "\n",
        "# Get the dimensions of the image\n",
        "height, width, channels = image.shape\n",
        "\n",
        "# Print the dimensions\n",
        "print(f\"Image dimensions: {height}x{width}x{channels}\")\n"
      ],
      "metadata": {
        "id": "Gk5QqjJUx5rS",
        "outputId": "ef92749a-5454-4337-bcd0-8412028892a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image dimensions: 640x640x3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize dataset to scale pixel values to a common scale\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def normalize_image(image):\n",
        "\n",
        "  mean = np.mean(image)\n",
        "  std = np.std(image)\n",
        "\n",
        "  normalized_image = (image - mean) / std\n",
        "\n",
        "  return normalized_image\n",
        "\n",
        "# Normalize all images in the combined directory\n",
        "for filename in os.listdir(totalDir):\n",
        "  if filename.endswith('.jpg'):\n",
        "    image = cv2.imread(os.path.join(totalDir, filename))\n",
        "    normalized_image = normalize_image(image)\n",
        "    cv2.imwrite(os.path.join(totalDir, filename), normalized_image)\n"
      ],
      "metadata": {
        "id": "vi_13iPtthMj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split the dataset"
      ],
      "metadata": {
        "id": "Tk3E7cOQunOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: split the dataset into 70/30 without creating a new directory. use sklearn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Get a list of all the image filenames in the combined directory\n",
        "image_filenames = [filename for filename in os.listdir(totalDir) if filename.endswith('.jpg')]\n",
        "\n",
        "# Split the filenames into train and test sets, with a 70/30 split\n",
        "train_filenames, test_filenames = train_test_split(image_filenames, test_size=0.3, random_state=42)\n",
        "\n",
        "# No need to create new directories, as the images are already in the combined directory\n",
        "# You can now use the train_filenames and test_filenames lists to access the training and test images, respectively.\n"
      ],
      "metadata": {
        "id": "jiEXQxfZwmjD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a deep learning model to train on the dataset. this is a one class classification task\n",
        "\n",
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "  layers.Flatten(input_shape=(640, 640, 3)),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(64, activation='relu'),\n",
        "  layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "lbHpja5UxlhV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Investigate model input shape\n",
        "model.input_shape"
      ],
      "metadata": {
        "id": "nv6_sVnnyhPj",
        "outputId": "d392de76-d210-4d82-d028-63fc8c823f6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 640, 640, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#investigate shpe of training data\n",
        "train_filenames[0].shape"
      ],
      "metadata": {
        "id": "7z0HI471ylnN",
        "outputId": "ade94f82-f625-460e-8a5d-9facb10f5839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-2850554d2335>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#investigate shpe of training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_filenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_filenames, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(test_filenames)\n",
        "\n",
        "# Save the model\n",
        "model.save('my_model.h5')"
      ],
      "metadata": {
        "id": "pmezNXdHyfsz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN9EjYZnZFVVuS/F00FE7SF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}